# -*- coding: utf-8 -*-
from __future__ import print_function
from sklearn import metrics
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
import numpy as np
import pickle
import pandas as pd
from pyvi import ViTokenizer
import re
from sklearn.feature_extraction.text import TfidfTransformer
from bs4 import BeautifulSoup

import string
import codecs

#T·ª´ ƒëi·ªÉn t√≠ch c·ª±c, ti√™u c·ª±c, ph·ªß ƒë·ªãnh
path_nag = 'sentiment_dicts/nag.txt'
path_pos = 'sentiment_dicts/pos.txt'
path_not = 'sentiment_dicts/not.txt'
path_spos = 'sentiment_dicts/spos.txt'
path_snot = 'sentiment_dicts/snot.txt'

with codecs.open(path_nag, 'r', encoding='UTF-8') as f:
    nag = f.readlines()
nag_list = [n.replace('\n', '') for n in nag]

with codecs.open(path_pos, 'r', encoding='UTF-8') as f:
    pos = f.readlines()
pos_list = [n.replace('\n', '') for n in pos]
with codecs.open(path_not, 'r', encoding='UTF-8') as f:
    not_ = f.readlines()
not_list = [n.replace('\n', '') for n in not_]


with codecs.open(path_spos, 'r', encoding='UTF-8') as f:
    spos = f.readlines()
spos_list = [n.rstrip() for n in spos]

with codecs.open(path_snot, 'r', encoding='UTF-8') as f:
    snot_ = f.readlines()
snot_list = [n.rstrip() for n in snot_]


def predict_lgr(model,text):
    pre_proba = model.predict_proba([text])[0]
    index = np.argmax(pre_proba)
    
    acc = round(pre_proba[index]*100,2)
   
    what = 'positive' if index == 1 else 'negative'
    if acc < 60:
      what = 'neutral'
    acc = str(acc) + "%"
    return acc,what


def load_model_lgr(filename = 'logistic_train_1h_15.pkl' ):
    classifier_filename_exp = filename
    with open(classifier_filename_exp, 'rb') as infile:
        model = pickle.load(infile)
    return model

def load_model_svm(filename = 'sentimentmodel.pkl'):
    classifier_filename_exp = filename
    with open(classifier_filename_exp, 'rb') as infile:
        model = pickle.load(infile)
    return model

def normalize_text(text):
    
    #Remove c√°c k√Ω t·ª± k√©o d√†i: vd: ƒë·∫πppppppp
    text = re.sub(r'([A-Z])\1+', lambda m: m.group(1).upper(), text, flags=re.IGNORECASE)

    # Chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng
    text = text.lower()

    #Chu·∫©n h√≥a ti·∫øng Vi·ªát, x·ª≠ l√Ω emoj, chu·∫©n h√≥a ti·∫øng Anh, thu·∫≠t ng·ªØ
    replace_list = {
        '√≤a': 'o√†', '√≥a': 'o√°', '·ªèa': 'o·∫£', '√µa': 'o√£', '·ªça': 'o·∫°', '√≤e': 'o√®', '√≥e': 'o√©','·ªèe': 'o·∫ª',
        '√µe': 'o·∫Ω', '·ªçe': 'o·∫π', '√πy': 'u·ª≥', '√∫y': 'u√Ω', '·ªßy': 'u·ª∑', '≈©y': 'u·ªπ','·ª•y': 'u·ªµ', 'u·∫£': '·ªßa',
        'aÃâ': '·∫£', '√¥ÃÅ': '·ªë', 'u¬¥': '·ªë','√¥ÃÉ': '·ªó', '√¥ÃÄ': '·ªì', '√¥Ãâ': '·ªï', '√¢ÃÅ': '·∫•', '√¢ÃÉ': '·∫´', '√¢Ãâ': '·∫©',
        '√¢ÃÄ': '·∫ß', 'oÃâ': '·ªè', '√™ÃÄ': '·ªÅ','√™ÃÉ': '·ªÖ', 'ƒÉÃÅ': '·∫Ø', 'uÃâ': '·ªß', '√™ÃÅ': '·∫ø', '∆°Ãâ': '·ªü', 'iÃâ': '·ªâ',
        'eÃâ': '·∫ª', '√†k': u' √† ','aÀã': '√†', 'iÀã': '√¨', 'ƒÉ¬¥': '·∫Ø','∆∞Ãâ': '·ª≠', 'eÀú': '·∫Ω', 'yÀú': '·ªπ', 'a¬¥': '√°',
        #Quy c√°c icon v·ªÅ 2 lo·∫°i emoj: T√≠ch c·ª±c ho·∫∑c ti√™u c·ª±c
        "üëπ": "nagative", "üëª": "positive", "üíÉ": "positive",'ü§ô': ' positive ', 'üëç': ' positive ',
        "üíÑ": "positive", "üíé": "positive", "üí©": "positive","üòï": "nagative", "üò±": "nagative", "üò∏": "positive",
        "üòæ": "nagative", "üö´": "nagative",  "ü§¨": "nagative","üßö": "positive", "üß°": "positive",'üê∂':' positive ',
        'üëé': ' nagative ', 'üò£': ' nagative ','‚ú®': ' positive ', '‚ù£': ' positive ','‚òÄ': ' positive ',
        '‚ô•': ' positive ', 'ü§©': ' positive ', 'like': ' positive ', 'üíå': ' positive ',
        'ü§£': ' positive ', 'üñ§': ' positive ', 'ü§§': ' positive ', ':(': ' nagative ', 'üò¢': ' nagative ',
        '‚ù§': ' positive ', 'üòç': ' positive ', 'üòò': ' positive ', 'üò™': ' nagative ', 'üòä': ' positive ',
        '?': ' ? ', 'üòÅ': ' positive ', 'üíñ': ' positive ', 'üòü': ' nagative ', 'üò≠': ' nagative ',
        'üíØ': ' positive ', 'üíó': ' positive ', '‚ô°': ' positive ', 'üíú': ' positive ', 'ü§ó': ' positive ',
        '^^': ' positive ', 'üò®': ' nagative ', '‚ò∫': ' positive ', 'üíã': ' positive ', 'üëå': ' positive ',
        'üòñ': ' nagative ', 'üòÄ': ' positive ', ':((': ' nagative ', 'üò°': ' nagative ', 'üò†': ' nagative ',
        'üòí': ' nagative ', 'üôÇ': ' positive ', 'üòè': ' nagative ', 'üòù': ' positive ', 'üòÑ': ' positive ',
        'üòô': ' positive ', 'üò§': ' nagative ', 'üòé': ' positive ', 'üòÜ': ' positive ', 'üíö': ' positive ',
        '‚úå': ' positive ', 'üíï': ' positive ', 'üòû': ' nagative ', 'üòì': ' nagative ', 'Ô∏èüÜóÔ∏è': ' positive ',
        'üòâ': ' positive ', 'üòÇ': ' positive ', ':v': '  positive ', '=))': '  positive ', 'üòã': ' positive ',
        'üíì': ' positive ', 'üòê': ' nagative ', ':3': ' positive ', 'üò´': ' nagative ', 'üò•': ' nagative ',
        'üòÉ': ' positive ', 'üò¨': ' üò¨ ', 'üòå': ' üòå ', 'üíõ': ' positive ', 'ü§ù': ' positive ', 'üéà': ' positive ',
        'üòó': ' positive ', 'ü§î': ' nagative ', 'üòë': ' nagative ', 'üî•': ' nagative ', 'üôè': ' nagative ',
        'üÜó': ' positive ', 'üòª': ' positive ', 'üíô': ' positive ', 'üíü': ' positive ',
        'üòö': ' positive ', '‚ùå': ' nagative ', 'üëè': ' positive ', ';)': ' positive ', '<3': ' positive ',
        'üåù': ' positive ',  'üå∑': ' positive ', 'üå∏': ' positive ', 'üå∫': ' positive ',
        'üåº': ' positive ', 'üçì': ' positive ', 'üêÖ': ' positive ', 'üêæ': ' positive ', 'üëâ': ' positive ',
        'üíê': ' positive ', 'üíû': ' positive ', 'üí•': ' positive ', 'üí™': ' positive ',
        'üí∞': ' positive ',  'üòá': ' positive ', 'üòõ': ' positive ', 'üòú': ' positive ',
        'üôÉ': ' positive ', 'ü§ë': ' positive ', 'ü§™': ' positive ','‚òπ': ' nagative ',  'üíÄ': ' nagative ',
        'üòî': ' nagative ', 'üòß': ' nagative ', 'üò©': ' nagative ', 'üò∞': ' nagative ', 'üò≥': ' nagative ',
        'üòµ': ' nagative ', 'üò∂': ' nagative ', 'üôÅ': ' nagative ',
        #Chu·∫©n h√≥a 1 s·ªë sentiment words/English words
        ':))': '  positive ', ':)': ' positive ', '√¥ k√™i': ' ok ', 'okie': ' ok ', ' o k√™ ': ' ok ',
        'okey': ' ok ', '√¥k√™': ' ok ', 'oki': ' ok ', ' oke ':  ' ok ',' okay':' ok ','ok√™':' ok ',
        ' tks ': u' c√°m ∆°n ', 'thks': u' c√°m ∆°n ', 'thanks': u' c√°m ∆°n ', 'ths': u' c√°m ∆°n ', 'thank': u' c√°m ∆°n ',
        '‚≠ê': 'star ', '*': 'star ', 'üåü': 'star ', 'üéâ': u' positive ','ko': u'kh√¥ng',
        'kg ': u' kh√¥ng ','not': u' kh√¥ng ', u' kg ': u' kh√¥ng ', '"k ': u' kh√¥ng ',' kh ':u' kh√¥ng ','k√¥':u' kh√¥ng ','hok':u' kh√¥ng ',' kp ': u' kh√¥ng ph·∫£i ',u' k√¥ ': u' kh√¥ng ', '"ko ': u' kh√¥ng ', u' ko ': u' kh√¥ng ', u' k ': u' kh√¥ng ', 'khong': u' kh√¥ng ', u' hok ': u' kh√¥ng ',
        'he he': ' positive ','hehe': ' positive ','hihi': ' positive ', 'haha': ' positive ', 'hjhj': ' positive ',
        ' lol ': ' nagative ',' cc ': ' nagative ','cute': u' d·ªÖ th∆∞∆°ng ','huhu': ' nagative ', ' vs ': u' v·ªõi ', 'wa': ' qu√° ', 'w√°': u' qu√°', 'j': u' g√¨ ', '‚Äú': ' ',
        ' sz ': u' c·ª° ', 'size': u' c·ª° ', u' ƒëx ': u' ƒë∆∞·ª£c ', 'dk': u' ƒë∆∞·ª£c ', 'dc': u' ƒë∆∞·ª£c ', 'ƒëk': u' ƒë∆∞·ª£c ',
        'ƒëc': u' ƒë∆∞·ª£c ','authentic': u' chu·∫©n ch√≠nh h√£ng ',u' aut ': u' chu·∫©n ch√≠nh h√£ng ', u' auth ': u' chu·∫©n ch√≠nh h√£ng ', 'thick': u' positive ', 'store': u' c·ª≠a h√†ng ',
        'shop': u' c·ª≠a h√†ng ', 'sp': u' s·∫£n ph·∫©m ', 'gud': u' t·ªët ','god': u' t·ªët ','wel done':' t·ªët ', 'good': u' t·ªët ', 'g√∫t': u' t·ªët ',
        's·∫•u': u' x·∫•u ','gut': u' t·ªët ', u' tot ': u' t·ªët ', u' nice ': u' t·ªët ', 'perfect': 'r·∫•t t·ªët', 'bt': u' b√¨nh th∆∞·ªùng ',
        'time': u' th·ªùi gian ', 'q√°': u' qu√° ', u' ship ': u' giao h√†ng ', u' m ': u' m√¨nh ', u' mik ': u' m√¨nh ',
        '√™Ãâ': '·ªÉ', 'product': 's·∫£n ph·∫©m', 'quality': 'ch·∫•t l∆∞·ª£ng','chat':' ch·∫•t ', 'excelent': 'ho√†n h·∫£o', 'bad': 't·ªá','fresh': ' t∆∞∆°i ','sad': ' t·ªá ',
        'date': u' h·∫°n s·ª≠ d·ª•ng ', 'hsd': u' h·∫°n s·ª≠ d·ª•ng ','quickly': u' nhanh ', 'quick': u' nhanh ','fast': u' nhanh ','delivery': u' giao h√†ng ',u' s√≠p ': u' giao h√†ng ',
        'beautiful': u' ƒë·∫πp tuy·ªát v·ªùi ', u' tl ': u' tr·∫£ l·ªùi ', u' r ': u' r·ªìi ', u' shopE ': u' c·ª≠a h√†ng ',u' order ': u' ƒë·∫∑t h√†ng ',
        'ch·∫•t lg': u' ch·∫•t l∆∞·ª£ng ',u' sd ': u' s·ª≠ d·ª•ng ',u' dt ': u' ƒëi·ªán tho·∫°i ',u' nt ': u' nh·∫Øn tin ',u' tl ': u' tr·∫£ l·ªùi ',u' s√†i ': u' x√†i ',u'bjo':u' bao gi·ªù ',
        'thik': u' th√≠ch ',u' sop ': u' c·ª≠a h√†ng ', ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': u' r·∫•t ',u'qu·∫£ ng ':u' qu·∫£ng  ',
        'dep': u' ƒë·∫πp ',u' xau ': u' x·∫•u ','delicious': u' ngon ', u'h√†g': u' h√†ng ', u'q·ªßa': u' qu·∫£ ',
        'iu': u' y√™u ','fake': u' gi·∫£ m·∫°o ', 'trl': 'tr·∫£ l·ªùi', '><': u' positive ',
        ' por ': u' t·ªá ',' poor ': u' t·ªá ', 'ib':u' nh·∫Øn tin ', 'rep':u' tr·∫£ l·ªùi ',u'fback':' feedback ','fedback':' feedback ',
        #d∆∞·ªõi 3* quy v·ªÅ 1*, tr√™n 3* quy v·ªÅ 5*
        '6 sao': ' 5star ','6 star': ' 5star ', '5star': ' 5star ','5 sao': ' 5star ','5sao': ' 5star ',
        'starstarstarstarstar': ' 5star ', '1 sao': ' 1star ', '1sao': ' 1star ','2 sao':' 1star ','2sao':' 1star ','1 star': '1star',
        '2 starstar':' 1star ','1star': ' 1star ', '0 sao': ' 1star ', '0star': ' 1star ',}

    for k, v in replace_list.items():
        text = text.replace(k, v)

    # chuyen punctuation th√†nh space
    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))
    text = text.translate(translator)
    text = re.sub(r"http\S+", "", text)
    text = BeautifulSoup(text, 'lxml').get_text()
    text = re.sub("\S*\d\S*", "", text).strip()

    

    """
    str =  " Th·ªùi gian giao h√†ng r·∫•t nhanh gi√° r·∫ª m√† gi√†y c·ª±c ch·∫•t! √äm ch√¢n l·∫Øm.thanks shop nhi·ªÅu" 
    text = ViTokenizer.tokenize(str) "Th·ªùi_gian giao h√†ng r·∫•t nhanh gi√° r·∫ª m√† gi√†y c·ª±c ch·∫•t ! √äm ch√¢n l·∫Øm . thanks shop nhi·ªÅu" 
    texts = text.split() ['Th·ªùi_gian', 'giao', 'h√†ng', 'r·∫•t', 'nhanh', 'gi√°', 'r·∫ª', 'm√†', 'gi√†y', 'c·ª±c', 'ch·∫•t', '!', '√äm', 'ch√¢n', 'l·∫Øm', '.', 'thanks', 'shop', 'nhi·ªÅu']
    texts = [t.replace('_', ' ') for t in texts] ['Th·ªùi gian', 'giao', 'h√†ng', 'r·∫•t', 'nhanh', 'gi√°', 'r·∫ª', 'm√†', 'gi√†y', 'c·ª±c', 'ch·∫•t', '!', '√äm', 'ch√¢n', 'l·∫Øm', '.', 'thanks', 'shop', 'nhi·ªÅu']
    """
    text = ViTokenizer.tokenize(text) 
    texts = text.split()
    len_text = len(texts)

    texts = [t.replace('_', ' ') for t in texts]
    for i in range(len_text):
        cp_text = texts[i]
        if cp_text in not_list: # X·ª≠ l√Ω v·∫•n ƒë·ªÅ ph·ªß ƒë·ªãnh (VD: √°o n√†y ch·∫≥ng ƒë·∫πp--> √°o n√†y notpos)
            numb_word = 2 if len_text - i - 1 >= 4 else len_text - i - 1

            for j in range(numb_word):
                if texts[i + j + 1] in pos_list:
                    texts[i] = 'notpos'
                    texts[i + j + 1] = ''

                if texts[i + j + 1] in nag_list:
                    texts[i] = 'notnag'
                    texts[i + j + 1] = ''
        else: #Th√™m feature cho nh·ªØng sentiment words (√°o n√†y ƒë·∫πp--> √°o n√†y ƒë·∫πp positive)
            if cp_text in pos_list:
                texts.append('positive')
            elif cp_text in nag_list:
                texts.append('nagative')

    text = u' '.join(texts)

    #remove n·ªët nh·ªØng k√Ω t·ª± th·ª´a th√£i
    text = text.replace(u'"', u' ')
    text = text.replace(u'Ô∏è', u'')
    text = text.replace('üèª','')
    for pos in spos_list:
      if pos in text:
        text += " " + 'positive'
    for snot in snot_list:
      if snot in text:
        text += " " + 'nagative'
    return text